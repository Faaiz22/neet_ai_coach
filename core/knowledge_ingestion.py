# -*- coding: utf-8 -*-
"""knowledge_ingestion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CTkh-DsYx9N00ZjMYlv85VIJha7AgkBT
"""

import PyPDF2
from docx import Document
import pandas as pd
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
import re
from typing import List, Dict
import os

class KnowledgeIngestion:
    def __init__(self):
        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)
        self.chroma_client = chromadb.PersistentClient(
            path=str(Config.KNOWLEDGE_BASE_DIR)
        )
        self.collection = None
        self.setup_collection()

    def setup_collection(self):
        """Setup ChromaDB collection"""
        try:
            self.collection = self.chroma_client.get_or_create_collection(
                name="neet_knowledge_base",
                metadata={"description": "NEET study materials and MCQs"}
            )
        except Exception as e:
            print(f"Error setting up collection: {e}")

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from PDF file"""
        try:
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
                return text
        except Exception as e:
            print(f"Error reading PDF {pdf_path}: {e}")
            return ""

    def extract_text_from_docx(self, docx_path: str) -> str:
        """Extract text from DOCX file"""
        try:
            doc = Document(docx_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except Exception as e:
            print(f"Error reading DOCX {docx_path}: {e}")
            return ""

    def chunk_text(self, text: str, chunk_size: int = None) -> List[str]:
        """Split text into chunks"""
        chunk_size = chunk_size or Config.CHUNK_SIZE
        overlap = Config.CHUNK_OVERLAP

        # Simple sentence-based chunking
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "

        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def process_ncert_files(self):
        """Process all NCERT files"""
        print("Processing NCERT files...")
        ncert_files = list(Config.NCERT_DIR.glob("**/*.pdf")) + list(Config.NCERT_DIR.glob("**/*.docx"))

        for file_path in ncert_files:
            print(f"Processing: {file_path.name}")

            # Extract text
            if file_path.suffix.lower() == '.pdf':
                text = self.extract_text_from_pdf(str(file_path))
            elif file_path.suffix.lower() == '.docx':
                text = self.extract_text_from_docx(str(file_path))
            else:
                continue

            if not text.strip():
                continue

            # Chunk text
            chunks = self.chunk_text(text)

            # Add to vector database
            for i, chunk in enumerate(chunks):
                doc_id = f"ncert_{file_path.stem}_{i}"

                try:
                    self.collection.add(
                        documents=[chunk],
                        metadatas=[{
                            "source": str(file_path),
                            "type": "ncert",
                            "subject": self._detect_subject(file_path.name),
                            "chunk_id": i
                        }],
                        ids=[doc_id]
                    )
                except Exception as e:
                    print(f"Error adding chunk {doc_id}: {e}")

    def process_mcq_files(self):
        """Process MCQ files"""
        print("Processing MCQ files...")
        mcq_files = list(Config.MCQ_DIR.glob("**/*.csv")) + list(Config.MCQ_DIR.glob("**/*.xlsx"))

        for file_path in mcq_files:
            print(f"Processing: {file_path.name}")

            try:
                # Read MCQ file
                if file_path.suffix.lower() == '.csv':
                    df = pd.read_csv(file_path)
                else:
                    df = pd.read_excel(file_path)

                # Expected columns: question, option_a, option_b, option_c, option_d, correct_answer, explanation
                required_cols = ['question', 'correct_answer']
                if not all(col in df.columns for col in required_cols):
                    print(f"Skipping {file_path.name}: Missing required columns")
                    continue

                for idx, row in df.iterrows():
                    question = str(row['question'])
                    options = []

                    # Get options
                    for opt_col in ['option_a', 'option_b', 'option_c', 'option_d']:
                        if opt_col in df.columns and pd.notna(row[opt_col]):
                            options.append(str(row[opt_col]))

                    # Create MCQ document
                    mcq_text = f"Question: {question}\n"
                    mcq_text += f"Options: {' | '.join(options)}\n"
                    mcq_text += f"Correct Answer: {row['correct_answer']}\n"

                    if 'explanation' in df.columns and pd.notna(row['explanation']):
                        mcq_text += f"Explanation: {row['explanation']}"

                    doc_id = f"mcq_{file_path.stem}_{idx}"

                    try:
                        self.collection.add(
                            documents=[mcq_text],
                            metadatas=[{
                                "source": str(file_path),
                                "type": "mcq",
                                "subject": self._detect_subject(file_path.name),
                                "question_id": idx
                            }],
                            ids=[doc_id]
                        )
                    except Exception as e:
                        print(f"Error adding MCQ {doc_id}: {e}")

            except Exception as e:
                print(f"Error processing {file_path}: {e}")

    def _detect_subject(self, filename: str) -> str:
        """Detect subject from filename"""
        filename_lower = filename.lower()
        if any(word in filename_lower for word in ['physics', 'phy']):
            return 'Physics'
        elif any(word in filename_lower for word in ['chemistry', 'chem']):
            return 'Chemistry'
        elif any(word in filename_lower for word in ['biology', 'bio', 'zoology', 'botany']):
            return 'Biology'
        else:
            return 'General'

    def search_knowledge_base(self, query: str, num_results: int = 5, subject_filter: str = None) -> List[Dict]:
        """Search the knowledge base"""
        try:
            where_clause = {}
            if subject_filter:
                where_clause["subject"] = subject_filter

            results = self.collection.query(
                query_texts=[query],
                n_results=num_results,
                where=where_clause if where_clause else None
            )

            search_results = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    result = {
                        'content': doc,
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else 0
                    }
                    search_results.append(result)

            return search_results

        except Exception as e:
            print(f"Error searching knowledge base: {e}")
            return []

    def ingest_all(self):
        """Ingest all knowledge sources"""
        print("Starting knowledge ingestion...")
        Config.ensure_directories()

        # Process NCERT files
        if Config.NCERT_DIR.exists() and any(Config.NCERT_DIR.iterdir()):
            self.process_ncert_files()
        else:
            print("No NCERT files found. Place PDF/DOCX files in data/ncert/")

        # Process MCQ files
        if Config.MCQ_DIR.exists() and any(Config.MCQ_DIR.iterdir()):
            self.process_mcq_files()
        else:
            print("No MCQ files found. Place CSV/XLSX files in data/mcqs/")

        print("Knowledge ingestion completed!")