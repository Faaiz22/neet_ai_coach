# -*- coding: utf-8 -*-
"""llm_plugin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CTkh-DsYx9N00ZjMYlv85VIJha7AgkBT
"""

import requests
import json
from typing import List, Dict, Optional
import subprocess
import sys

class LLMPlugin:
    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.DEFAULT_MODEL
        self.ollama_base_url = "http://localhost:11434"
        self.session_id = None

    def check_ollama_running(self) -> bool:
        """Check if Ollama service is running"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

    def start_ollama(self):
        """Start Ollama service"""
        try:
            subprocess.run(["ollama", "serve"], check=False, capture_output=True)
        except Exception as e:
            print(f"Could not start Ollama automatically: {e}")
            print("Please start Ollama manually with: ollama serve")

    def pull_model(self, model_name: str = None) -> bool:
        """Pull model if not available"""
        model = model_name or self.model_name
        try:
            subprocess.run(["ollama", "pull", model], check=True, capture_output=True)
            return True
        except subprocess.CalledProcessError:
            return False

    def list_available_models(self) -> List[str]:
        """Get list of available models"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
        except:
            pass
        return []

    def generate_response(self, prompt: str, context: str = "", system_prompt: str = "") -> str:
        """Generate response using Ollama"""
        if not self.check_ollama_running():
            self.start_ollama()
            if not self.check_ollama_running():
                return "Error: Ollama service is not running. Please start it with 'ollama serve'"

        # Construct full prompt
        full_prompt = ""
        if system_prompt:
            full_prompt += f"System: {system_prompt}\n\n"
        if context:
            full_prompt += f"Context: {context}\n\n"
        full_prompt += f"Human: {prompt}\n\nAssistant:"

        try:
            payload = {
                "model": self.model_name,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": Config.TEMPERATURE,
                    "num_predict": Config.MAX_TOKENS
                }
            }

            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json=payload,
                timeout=60
            )

            if response.status_code == 200:
                return response.json().get("response", "No response generated")
            else:
                return f"Error: {response.status_code} - {response.text}"

        except Exception as e:
            return f"Error generating response: {str(e)}"

    def generate_explanation(self, topic: str, context: str = "") -> str:
        """Generate detailed explanation for a topic"""
        system_prompt = """You are an expert NEET coach specializing in Physics, Chemistry, and Biology.
        Provide clear, detailed explanations with examples. Use simple language but maintain scientific accuracy.
        Include relevant formulas, diagrams descriptions, and memory aids when helpful."""

        prompt = f"Explain the topic: {topic}. Make it comprehensive yet easy to understand for NEET preparation."

        return self.generate_response(prompt, context, system_prompt)

    def solve_mcq(self, question: str, options: List[str], context: str = "") -> str:
        """Solve MCQ with detailed explanation"""
        system_prompt = """You are a NEET expert. Solve the MCQ step by step.
        1. Analyze the question
        2. Apply relevant concepts
        3. Eliminate wrong options
        4. Explain the correct answer
        5. Provide tips to remember the concept"""

        options_text = "\n".join([f"{chr(65+i)}. {opt}" for i, opt in enumerate(options)])
        prompt = f"Question: {question}\n\nOptions:\n{options_text}\n\nSolve this step by step."

        return self.generate_response(prompt, context, system_prompt)